{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "    # Read data from data_path\n",
    "    #  name       : string,  name of data (train: span, question, question.id, context, answer),\n",
    "    #                                      (test: question, question.id, context)\n",
    "    #  with_label : bool,  read data with label or without label\n",
    "\n",
    "    def add_data(self, name,  data_path,  with_label=True):\n",
    "        print ('read data from %s...'%data_path)\n",
    "        train_files = {'train.question.id': [],\n",
    "                        'train.question':[] ,\n",
    "                        'train.context': [],\n",
    "                        'train.answer': [],\n",
    "                        'train.span': []}\n",
    "\n",
    "        test_files = {'test.context': [],\n",
    "                        'test.question': [],\n",
    "                        'test.question.id': []}\n",
    "\n",
    "        file_path = ''\n",
    "        if with_label:\n",
    "            dic = train_files\n",
    "        else:\n",
    "            dic = test_files\n",
    "        for filename in dic.keys():\n",
    "            file_path = data_path + '/' + filename\n",
    "            with open(file_path, 'r') as f:\n",
    "                dic[filename] = f.read().splitlines()\n",
    "\n",
    "        if with_label:\n",
    "            self.data[name] = train_files\n",
    "        else:\n",
    "            self.data[name] = test_files\n",
    "\n",
    "    # Build dictionary\n",
    "    #  vocab_size : maximum number of word in yout dictionary\n",
    "    def sequence2matrix(self,  word2vec_model):\n",
    "        question_size = 20\n",
    "        context_size = 300\n",
    "        word_vec_size = 250\n",
    "        print ('sequence to matrix ing...')\n",
    "        for key in self.data.keys():\n",
    "            print (\"key in data: \", key)\n",
    "            if key == 'train_data' or key == 'test_data':\n",
    "                for kee in self.data[key].keys():\n",
    "                    print ('converting %s: %s to vec...' % (key, kee))\n",
    "                    texts = self.data[key][kee]\n",
    "                    \n",
    "                    if kee[-7:] == 'context':\n",
    "                    \n",
    "                        context_matrix = []\n",
    "                        \n",
    "                        for context in texts:\n",
    "                            tmp = []\n",
    "                            words = list(jieba.cut(context, cut_all=False))\n",
    "                            for i in range(context_size):\n",
    "                                if i < len(words):\n",
    "                                    try:\n",
    "                                        tmp.append(word2vec_model[words[i]])\n",
    "                                    except KeyError:\n",
    "                                        #print (\"%s not found\" % word)\n",
    "                                        tmp.append(np.zeros(shape=(word_vec_size,)))\n",
    "                                else:\n",
    "                                    tmp.append(np.zeros(shape=(word_vec_size,)))\n",
    "                                    \n",
    "                            context_matrix.append(np.array(tmp))\n",
    "\n",
    "                        context_matrix = np.array(context_matrix)\n",
    "                        print (context_matrix)\n",
    "                        print ('context.shape: ' + str(context_matrix.shape))\n",
    "                        if key == 'train_data':\n",
    "                            self.train_context_matrix = context_matrix\n",
    "                            np.save('save/train_context_matrix.npy', self.train_context_matrix)\n",
    "                        else:\n",
    "                            self.test_context_matrix = context_matrix\n",
    "                            np.save('save/test_context_matrix.npy', self.train_context_matrix)\n",
    "                    elif kee[-8:] == 'question':\n",
    "                        question_matrix = []\n",
    "                        for question in texts:\n",
    "                            tmp = []\n",
    "                            words = list(jieba.cut(question, cut_all=False))\n",
    "                            for i in range(question_size):\n",
    "                                if i < len(words):\n",
    "                                    try:\n",
    "                                        tmp.append(word2vec_model[words[i]])\n",
    "                                    except KeyError:\n",
    "                                        #print (\"%s not found\" % word)\n",
    "                                        tmp.append(np.zeros(shape=(word_vec_size,)))\n",
    "                                else:\n",
    "                                    tmp.append(np.zeros(shape=(word_vec_size,)))\n",
    "                                    \n",
    "                            question_matrix.append(np.array(tmp))\n",
    "\n",
    "                        question_matrix = np.array(question_matrix)\n",
    "                        print (question_matrix)\n",
    "                        print ('question.shape: ' + str(question_matrix.shape))\n",
    "                        if key == 'train_data':\n",
    "                            self.train_question_matrix = question_matrix\n",
    "                            np.save('save/train_question_matrix.npy', self.train_question_matrix)\n",
    "                        else:\n",
    "                            self.test_question_matrix = question_matrix\n",
    "                            np.save('save/test_question_matrix.npy', self.test_question_matrix)     \n",
    "\n",
    "    # Save tokenizer to specified path\n",
    "    def save_tokenizer(self,  path):\n",
    "        print ('save tokenizer to %s'%path)\n",
    "        pk.dump(self.tokenizer,  open(path,  'wb'))\n",
    "\n",
    "    # Load tokenizer from specified path\n",
    "    def load_tokenizer(self, path):\n",
    "        print ('Load tokenizer from %s'%path)\n",
    "        self.tokenizer = pk.load(open(path,  'rb'))\n",
    "\n",
    "    # Convert words in data to index and pad to equal size\n",
    "    #  maxlen : max length after padding\n",
    "    def to_sequence(self,  maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        for key in self.data:\n",
    "            print ('Converting %s to sequences'%key)\n",
    "            tmp = self.tokenizer.texts_to_sequences(self.data[key][0])\n",
    "            self.data[key][0] = np.array(pad_sequences(tmp,  maxlen=maxlen))\n",
    "\n",
    "    # Convert texts in data to BOW feature\n",
    "    def to_bow(self):\n",
    "        for key in self.data:\n",
    "            print ('Converting %s to tfidf'%key)\n",
    "            self.data[key][0] = self.tokenizer.texts_to_matrix(self.data[key][0], mode='count')\n",
    "\n",
    "    # Convert label to category type,  call this function if use categorical loss\n",
    "    def to_category(self):\n",
    "        for key in self.data:\n",
    "            if len(self.data[key]) == 2:\n",
    "                self.data[key][1] = np.array(to_categorical(self.data[key][1]))\n",
    "\n",
    "    def get_semi_data(self, name, label, threshold, loss_function) :\n",
    "        # if th==0.3,  will pick label>0.7 and label<0.3\n",
    "        label = np.squeeze(label)\n",
    "        index = (label>1-threshold) + (label<threshold)\n",
    "        semi_X = self.data[name][0]\n",
    "        semi_Y = np.greater(label,  0.5).astype(np.int32)\n",
    "        if loss_function=='binary_crossentropy':\n",
    "            return semi_X[index, :],  semi_Y[index]\n",
    "        elif loss_function=='categorical_crossentropy':\n",
    "            return semi_X[index, :],  to_categorical(semi_Y[index])\n",
    "        else :\n",
    "            raise Exception('Unknown loss function : %s'%loss_function)\n",
    "\n",
    "    # get data by name\n",
    "    def get_data(self, name):\n",
    "        return self.data[name]\n",
    "\n",
    "    # split data to two part by a specified ratio\n",
    "    #  name  : string,  same as add_data\n",
    "    #  ratio : float,  ratio to split\n",
    "    def split_data(self,  name,  ratio):\n",
    "        data = self.data[name]\n",
    "        X = data[0]\n",
    "        Y = data[1]\n",
    "        data_size = len(X)\n",
    "        val_size = int(data_size * ratio)\n",
    "        return (X[val_size:], Y[val_size:]), (X[:val_size], Y[:val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "read data from feature...\n",
      "read data from feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence to matrix ing...\n",
      "key in data:  train_data\n",
      "converting train_data: train.question.id to vec...\n",
      "converting train_data: train.question to vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /var/folders/8d/zg5x_bgn10x7tmtsggfzcx000000gn/T/jieba.cache\n",
      "Loading model cost 1.140 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.42864755 -0.64748293  0.03303193 ..., -0.60561109  0.184689\n",
      "    0.44512257]\n",
      "  [-0.02595187 -0.06265915 -0.01351052 ..., -0.05188916 -0.05256062\n",
      "   -0.03191547]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[ 1.57752621  0.85066235  2.19910336 ...,  0.34219655  1.65544212\n",
      "   -0.12252838]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [-0.21273789  0.16108374  0.34231377 ..., -0.16986136  0.49472159\n",
      "    0.71275365]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.06774613 -0.52926499  0.26955238 ..., -0.27076551  0.20405914\n",
      "   -0.74388427]\n",
      "  [ 2.29042864 -2.2812345   0.57361645 ..., -0.0122174   1.26038313\n",
      "   -1.65129817]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " ..., \n",
      " [[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.22284214 -0.10041303  0.16743587 ..., -0.38680604 -0.12073102\n",
      "    0.52564973]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[-1.79448831 -0.03510478  2.01645184 ..., -0.88334554  2.39466691\n",
      "   -0.13757055]\n",
      "  [-0.91560429 -0.01009022 -0.7673412  ..., -0.58160901  2.20071769\n",
      "    1.05535984]\n",
      "  [ 1.32798266 -0.35110217 -0.73320562 ..., -1.68745852  1.34120333\n",
      "    0.96320373]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]]\n",
      "question.shape: (14611, 20, 250)\n",
      "converting train_data: train.context to vec...\n",
      "[[[-1.34709632 -0.57565546 -0.0973889  ..., -0.57510298  0.55289483\n",
      "   -1.25628781]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.61275947 -1.7391988   0.20329279 ..., -1.97416985  0.0867565\n",
      "   -1.06798089]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.79235321 -0.40139365  0.80382097 ...,  0.52274168  1.62431657\n",
      "    0.05716266]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[-1.34709632 -0.57565546 -0.0973889  ..., -0.57510298  0.55289483\n",
      "   -1.25628781]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.61275947 -1.7391988   0.20329279 ..., -1.97416985  0.0867565\n",
      "   -1.06798089]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.79235321 -0.40139365  0.80382097 ...,  0.52274168  1.62431657\n",
      "    0.05716266]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[-1.34709632 -0.57565546 -0.0973889  ..., -0.57510298  0.55289483\n",
      "   -1.25628781]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.61275947 -1.7391988   0.20329279 ..., -1.97416985  0.0867565\n",
      "   -1.06798089]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.79235321 -0.40139365  0.80382097 ...,  0.52274168  1.62431657\n",
      "    0.05716266]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " ..., \n",
      " [[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [-0.20702487 -0.28046933  0.2379373  ..., -0.39130703 -0.05204405\n",
      "    1.05538058]\n",
      "  [ 0.22284214 -0.10041303  0.16743587 ..., -0.38680604 -0.12073102\n",
      "    0.52564973]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [-0.20702487 -0.28046933  0.2379373  ..., -0.39130703 -0.05204405\n",
      "    1.05538058]\n",
      "  [ 0.22284214 -0.10041303  0.16743587 ..., -0.38680604 -0.12073102\n",
      "    0.52564973]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [-0.20702487 -0.28046933  0.2379373  ..., -0.39130703 -0.05204405\n",
      "    1.05538058]\n",
      "  [ 0.22284214 -0.10041303  0.16743587 ..., -0.38680604 -0.12073102\n",
      "    0.52564973]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]]\n",
      "context.shape: (14611, 300, 250)\n",
      "converting train_data: train.answer to vec...\n",
      "converting train_data: train.span to vec...\n",
      "key in data:  test_data\n",
      "converting test_data: test.context to vec...\n",
      "[[[-0.39040032  0.7109682   0.39208478 ..., -0.7375381   0.45857656\n",
      "    0.6708048 ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[-0.39040032  0.7109682   0.39208478 ..., -0.7375381   0.45857656\n",
      "    0.6708048 ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[-0.39040032  0.7109682   0.39208478 ..., -0.7375381   0.45857656\n",
      "    0.6708048 ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " ..., \n",
      " [[-0.92544568  0.32547829 -0.50916111 ..., -2.1213274   1.31012607\n",
      "    0.09664862]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [-0.38170144 -1.61103439 -1.10691381 ..., -0.63075817  1.20927966\n",
      "    1.32118607]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[-0.92544568  0.32547829 -0.50916111 ..., -2.1213274   1.31012607\n",
      "    0.09664862]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [-0.38170144 -1.61103439 -1.10691381 ..., -0.63075817  1.20927966\n",
      "    1.32118607]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]\n",
      "\n",
      " [[-0.92544568  0.32547829 -0.50916111 ..., -2.1213274   1.31012607\n",
      "    0.09664862]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [-0.38170144 -1.61103439 -1.10691381 ..., -0.63075817  1.20927966\n",
      "    1.32118607]\n",
      "  ..., \n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.         ...,  0.          0.          0.        ]]]\n",
      "context.shape: (1741, 300, 250)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import jieba\n",
    "#from util import DataManager\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys, argparse, os\n",
    "from gensim.models import word2vec\n",
    "from gensim import models\n",
    "import logging\n",
    "\n",
    "\n",
    "def main():\n",
    "    dm = DataManager()\n",
    "    print ('Loading data...')\n",
    "    dm.add_data('train_data', 'feature', True)\n",
    "    dm.add_data('test_data', 'feature', False)\n",
    "    \n",
    "\n",
    "    train_id = (dm.get_data('train_data')['train.question.id'])\n",
    "    train_q = (dm.get_data('train_data')['train.question'])\n",
    "    train_ans = (dm.get_data('train_data')['train.answer'])\n",
    "    train_con = (dm.get_data('train_data')['train.context'])\n",
    "    train_span = (dm.get_data('train_data')['train.span'])\n",
    "    \n",
    "    test_id = (dm.get_data('test_data')['test.question.id'])\n",
    "    test_q = (dm.get_data('test_data')['test.question'])\n",
    "    test_con = (dm.get_data('test_data')['test.context'])\n",
    "\n",
    "    word2vec_model = models.Word2Vec.load('save/med250.model.bin')\n",
    "    '''\n",
    "    for i in range(1):\n",
    "        print (train_con[i])\n",
    "    print (len(train_con))\n",
    "    \n",
    "    for word in train_con[0]:\n",
    "        try:\n",
    "            print (word2vec_model[word])\n",
    "        except KeyError:\n",
    "            print ('EEEEEEEE')\n",
    "    \n",
    "    l = [jieba.cut(sentence, cut_all=False) for sentence in train_con]\n",
    "    print ('|'.join(l[0]))\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    \n",
    "    vec_matrix = []\n",
    "    \n",
    "    try:\n",
    "        for context in test_con:\n",
    "           \n",
    "            tmp = []\n",
    "            words = list(jieba.cut(context, cut_all=False))\n",
    "            for i in range(300):\n",
    "                if i < len(words):\n",
    "                    try:\n",
    "                        tmp.append(word2vec_model[words[i]])\n",
    "                    except KeyError:\n",
    "                        #print (\"%s not found\" % word)\n",
    "                        tmp.append(np.zeros(shape=(250,)))\n",
    "                else:\n",
    "                    tmp.append(np.zeros(shape=(250,)))\n",
    "            tmp = np.array(tmp)\n",
    "            \n",
    "            vec_matrix.append(tmp)\n",
    "        vec_matrix = np.array(vec_matrix)\n",
    "        print (vec_matrix)\n",
    "        for i in range(100):\n",
    "            print (vec_matrix[i].shape)\n",
    "        \n",
    "    except KeyError:\n",
    "        pass\n",
    "    '''\n",
    "    \n",
    "\n",
    "    dm.sequence2matrix(word2vec_model)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
